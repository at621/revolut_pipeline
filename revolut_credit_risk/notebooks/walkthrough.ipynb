{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Model — DFS + MIV Pipeline Walkthrough\n",
    "\n",
    "Interactive walkthrough of the Revolut DFS + MIV credit scorecard methodology.\n",
    "\n",
    "This notebook replicates the methodology from *\"Enhancing Credit Risk Models at Revolut by Combining Deep Feature Synthesis and Marginal Information Value\"* (Spinella & Krisciunas, 2025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Pipeline modules\n",
    "from revolut_credit_risk import config\n",
    "from revolut_credit_risk.logging_config import setup_logging\n",
    "from revolut_credit_risk.data.synthetic_data import generate_synthetic_data, save_dataset\n",
    "from revolut_credit_risk.features.dfs_engine import run_dfs\n",
    "from revolut_credit_risk.features.binning import bin_features, transform_woe\n",
    "from revolut_credit_risk.features.variable_config import get_variable_configs\n",
    "from revolut_credit_risk.selection.information_value import collect_iv, filter_by_iv, bivariate_analysis\n",
    "from revolut_credit_risk.selection.miv_selector import run_miv_selection\n",
    "from revolut_credit_risk.model.scorecard import train_scorecard, benchmark_models\n",
    "from revolut_credit_risk.model.calibration import calibrate_isotonic\n",
    "from revolut_credit_risk.evaluation.metrics import compute_all_splits, plot_roc_curve, plot_lorenz_curve, plot_calibration_curve, plot_miv_selection\n",
    "\n",
    "setup_logging()\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Use smaller dataset for notebook speed\n",
    "config.N_CUSTOMERS = 2000\n",
    "config.DFS_MAX_FEATURES = 200\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "Generate synthetic relational data mimicking a digital bank's entity structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_synthetic_data()\n",
    "\n",
    "print(f'Customers:    {len(data.customers):,}')\n",
    "print(f'Accounts:     {len(data.accounts):,}')\n",
    "print(f'Transactions: {len(data.transactions):,}')\n",
    "print(f'Applications: {len(data.credit_applications):,}')\n",
    "print(f'Default rate: {data.loan_performance[\"is_default\"].mean():.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity relationship overview\n",
    "print('=== Customers ===')\n",
    "display(data.customers.head())\n",
    "print('\\n=== Accounts ===')\n",
    "display(data.accounts.head())\n",
    "print('\\n=== Transactions (sample) ===')\n",
    "display(data.transactions.head())\n",
    "print('\\n=== Credit Applications ===')\n",
    "display(data.credit_applications.head())\n",
    "print('\\n=== Loan Performance ===')\n",
    "display(data.loan_performance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default rate by customer demographics\n",
    "merged = data.loan_performance.merge(data.credit_applications[['application_id', 'customer_id']])\n",
    "merged = merged.merge(data.customers, on='customer_id')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "merged.groupby('income_band')['is_default'].mean().plot(kind='bar', ax=axes[0], title='Default Rate by Income')\n",
    "merged.groupby('employment_status')['is_default'].mean().plot(kind='bar', ax=axes[1], title='Default Rate by Employment')\n",
    "merged.groupby(pd.cut(merged['age'], bins=5))['is_default'].mean().plot(kind='bar', ax=axes[2], title='Default Rate by Age')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_ylabel('Default Rate')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Feature Synthesis [Paper §2.2.4]\n",
    "\n",
    "Automatically generate features from the relational data using featuretools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = run_dfs(data)\n",
    "print(f'Feature matrix shape: {feature_matrix.shape}')\n",
    "print(f'\\nSample feature names:')\n",
    "for name in feature_matrix.columns[:20]:\n",
    "    print(f'  {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with target and split\n",
    "target = data.loan_performance[['application_id', 'is_default']].copy()\n",
    "full = feature_matrix.join(target.set_index('application_id'), how='inner')\n",
    "\n",
    "y = full['is_default']\n",
    "X = full.drop(columns=['is_default'])\n",
    "\n",
    "# Time-based split\n",
    "app_dates = data.credit_applications.set_index('application_id')['application_date']\n",
    "app_dates = app_dates.loc[X.index]\n",
    "sorted_idx = app_dates.sort_values().index\n",
    "\n",
    "n = len(sorted_idx)\n",
    "n_train = int(n * config.TRAIN_RATIO)\n",
    "n_test = int(n * config.TEST_RATIO)\n",
    "\n",
    "train_idx = sorted_idx[:n_train]\n",
    "test_idx = sorted_idx[n_train:n_train + n_test]\n",
    "oot_idx = sorted_idx[n_train + n_test:]\n",
    "\n",
    "X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "X_test, y_test = X.loc[test_idx], y.loc[test_idx]\n",
    "X_oot, y_oot = X.loc[oot_idx], y.loc[oot_idx]\n",
    "\n",
    "print(f'Train: {len(X_train)}, Test: {len(X_test)}, OOT: {len(X_oot)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binning & WoE Transformation [Paper §2.2.5, §2.2.6]\n",
    "\n",
    "Coarse binning with optbinning and Weight of Evidence transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_configs = get_variable_configs(X_train.columns.tolist())\n",
    "binning_results = bin_features(X_train, y_train, var_configs)\n",
    "X_woe_train = transform_woe(X_train, binning_results)\n",
    "X_woe_test = transform_woe(X_test, binning_results)\n",
    "X_woe_oot = transform_woe(X_oot, binning_results)\n",
    "\n",
    "print(f'Binned features: {len(binning_results.results)}')\n",
    "print(f'WoE columns: {X_woe_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show binning table for the top feature by IV\n",
    "iv_table = collect_iv(binning_results)\n",
    "top_feat = iv_table.iloc[0]['feature']\n",
    "print(f'Top feature: {top_feat} (IV={iv_table.iloc[0][\"iv\"]:.4f})')\n",
    "display(binning_results.results[top_feat].binning_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IV Distribution & Bivariate Analysis [Paper §2.3.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV distribution histogram\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(iv_table['iv'], bins=30, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=0.02, color='red', linestyle='--', label='IV threshold (0.02)')\n",
    "ax.set_xlabel('Information Value')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('IV Distribution of DFS Features')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Bivariate analysis\n",
    "biv = bivariate_analysis(binning_results, X_woe_train, y_train)\n",
    "display(biv.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MIV Feature Selection [Paper §2.3.2, Fig. 5]\n",
    "\n",
    "Greedy forward selection using Marginal Information Value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = filter_by_iv(iv_table)\n",
    "miv_result = run_miv_selection(\n",
    "    X_woe_train, y_train, X_woe_test, y_test,\n",
    "    iv_table, binning_results, candidates,\n",
    ")\n",
    "\n",
    "print(f'Selected {len(miv_result.selected_features)} features:')\n",
    "for step in miv_result.steps:\n",
    "    print(f'  Step {step.step}: {step.feature_added} (MIV={step.miv:.4f}, AUC test={step.auc_test:.4f})')\n",
    "print(f'Stopping reason: {miv_result.stopping_reason}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate Paper Fig. 5: AUC vs step + MIV bar chart\n",
    "if miv_result.steps:\n",
    "    steps = miv_result.steps\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "    ax1.plot([s.step for s in steps], [s.auc_train for s in steps], 'b-o', label='Train')\n",
    "    ax1.plot([s.step for s in steps], [s.auc_test for s in steps], 'r-o', label='Test')\n",
    "    ax1.set_ylabel('ROC AUC')\n",
    "    ax1.set_title('MIV Feature Selection Progress [Paper Fig. 5]')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    ax2.bar([s.step for s in steps], [s.miv for s in steps], color='steelblue')\n",
    "    ax2.set_xlabel('Selection Step')\n",
    "    ax2.set_ylabel('MIV')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Model — Logistic Regression Scorecard [Paper §2.4.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = miv_result.selected_features\n",
    "scorecard = train_scorecard(X_woe_train, y_train, selected, binning_results)\n",
    "print(scorecard.summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient table\n",
    "display(scorecard.coefficient_table)\n",
    "\n",
    "# Scorecard points\n",
    "if scorecard.scorecard_points is not None:\n",
    "    display(scorecard.scorecard_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance [Paper §2.4.1, Fig. 6, Fig. 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woe_cols = [f'woe_{f}' for f in selected]\n",
    "prob_train = scorecard.model.predict(sm.add_constant(X_woe_train[woe_cols]))\n",
    "prob_test = scorecard.model.predict(sm.add_constant(X_woe_test[woe_cols]))\n",
    "prob_oot = scorecard.model.predict(sm.add_constant(X_woe_oot[woe_cols]))\n",
    "\n",
    "metrics_df = compute_all_splits(\n",
    "    y_train.values, prob_train,\n",
    "    y_test.values, prob_test,\n",
    "    y_oot.values, prob_oot,\n",
    ")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "for y_true, y_prob, label in [\n",
    "    (y_train.values, prob_train, 'Train'),\n",
    "    (y_test.values, prob_test, 'Test'),\n",
    "    (y_oot.values, prob_oot, 'OOT'),\n",
    "]:\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    ax.plot(fpr, tpr, lw=2, label=f'{label} (AUC={auc:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR')\n",
    "ax.set_title('ROC Curve')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lorenz Curve [Paper Fig. 6]\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "order = np.argsort(-prob_test)\n",
    "y_sorted = y_test.values[order]\n",
    "cum_bads = np.cumsum(y_sorted) / y_sorted.sum()\n",
    "x = np.arange(1, len(y_sorted) + 1) / len(y_sorted)\n",
    "\n",
    "ax.plot(x, cum_bads, lw=2, label='Model')\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax.set_xlabel('Fraction of Population')\n",
    "ax.set_ylabel('Cumulative Fraction of Defaults')\n",
    "ax.set_title('Lorenz Curve [Paper Fig. 6]')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PD Calibration [Paper §2.4.1, Fig. 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = calibrate_isotonic(prob_train, y_train.values, prob_test, y_test.values)\n",
    "print(f'Brier Score before: {cal.brier_before:.4f}')\n",
    "print(f'Brier Score after:  {cal.brier_after:.4f}')\n",
    "\n",
    "# Calibration curve [Paper Fig. 7]\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "for probs, label in [(prob_test, 'Before'), (cal.calibrated_probs_test, 'After')]:\n",
    "    n_bins = 10\n",
    "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_idx = np.clip(np.digitize(probs, bin_edges) - 1, 0, n_bins - 1)\n",
    "    obs, pred = [], []\n",
    "    for i in range(n_bins):\n",
    "        mask = bin_idx == i\n",
    "        if mask.sum() > 0:\n",
    "            obs.append(y_test.values[mask].mean())\n",
    "            pred.append(probs[mask].mean())\n",
    "    ax.plot(pred, obs, 'o-', label=label)\n",
    "\n",
    "ax.plot([0, 0.5], [0, 0.5], 'k--', label='Perfect')\n",
    "ax.set_xlabel('Predicted PD')\n",
    "ax.set_ylabel('Observed Default Rate')\n",
    "ax.set_title('Calibration Curve [Paper Fig. 7]')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Benchmarking [Paper §2.4.2, Table 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_auc_train = roc_auc_score(y_train, prob_train)\n",
    "lr_auc_test = roc_auc_score(y_test, prob_test)\n",
    "lr_auc_oot = roc_auc_score(y_oot, prob_oot)\n",
    "\n",
    "bench = benchmark_models(\n",
    "    X_train, y_train, X_test, y_test, X_oot, y_oot,\n",
    "    lr_auc_train, lr_auc_test, lr_auc_oot,\n",
    ")\n",
    "display(bench)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Residual Monitoring [Paper §3, Fig. 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from revolut_credit_risk.monitoring.residual_monitor import run_residual_monitoring\n",
    "\n",
    "monitor = run_residual_monitoring(\n",
    "    model_probs=prob_train,\n",
    "    y_true=y_train.values,\n",
    "    X_woe=X_woe_train,\n",
    "    X_raw=X_train,\n",
    "    selected_features=selected,\n",
    "    binning_results=binning_results,\n",
    ")\n",
    "\n",
    "if monitor.candidate_table is not None:\n",
    "    display(monitor.candidate_table.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Pipeline complete.** All results match the methodology from the Revolut DFS + MIV paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
