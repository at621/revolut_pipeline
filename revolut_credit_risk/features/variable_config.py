"""Per-variable binning configuration — LLM-assisted (optional) or defaults.

[Assumption] The entire per-variable configuration system is our engineering
design. The paper does not discuss tooling for variable configuration.
"""
from __future__ import annotations

import logging
from pathlib import Path

import yaml

from revolut_credit_risk import config

logger = logging.getLogger(__name__)

# Default binning parameters when LLM is not used
_DEFAULTS = {
    "dtype": "numerical",
    "monotonic_trend": config.BINNING_MONOTONIC_TREND,
    "max_n_bins": config.BINNING_MAX_N_BINS,
    "min_bin_size": config.BINNING_MIN_BIN_SIZE,
}


def get_variable_configs(
    feature_names: list[str],
    config_path: Path | None = None,
) -> dict[str, dict]:
    """Return per-variable binning configurations.

    Priority order:
    1. If config YAML file exists at *config_path*, load it.
    2. Else if ``config.USE_LLM_CONFIG`` is True, call the LLM.
    3. Else use sensible defaults.

    Parameters
    ----------
    feature_names : list[str]
        Names of DFS-generated features.
    config_path : Path, optional
        Path to YAML config file. Defaults to ``config.VARIABLE_CONFIG_PATH``.

    Returns
    -------
    dict[str, dict]
        Mapping from feature name to binning config dict.
    """
    config_path = config_path or config.VARIABLE_CONFIG_PATH

    # 1. Try loading existing config
    if config_path.exists():
        logger.info("Loading variable config from %s", config_path)
        return _load_config(config_path, feature_names)

    # 2. LLM-assisted mode
    if config.USE_LLM_CONFIG:
        logger.info("Generating variable config via LLM (%s)...", config.LLM_MODEL)
        var_configs = _generate_llm_config(feature_names)
        _save_config(var_configs, config_path)
        return var_configs

    # 3. Defaults
    logger.info(
        "Using default variable config for %d features (USE_LLM_CONFIG=False)",
        len(feature_names),
    )
    var_configs = {name: dict(_DEFAULTS) for name in feature_names}
    _save_config(var_configs, config_path)
    return var_configs


def _load_config(path: Path, feature_names: list[str]) -> dict[str, dict]:
    """Load YAML config and merge with defaults for missing features."""
    raw = yaml.safe_load(path.read_text(encoding="utf-8"))
    defaults = raw.get("_defaults", _DEFAULTS)
    variables = raw.get("variables", {})

    result: dict[str, dict] = {}
    for name in feature_names:
        cfg = dict(defaults)
        if name in variables:
            cfg.update(variables[name])
        result[name] = cfg

    logger.info(
        "Loaded config for %d features (%d with overrides)",
        len(result), len(variables),
    )
    return result


def _save_config(var_configs: dict[str, dict], path: Path) -> None:
    """Save variable config to YAML for human review and reproducibility."""
    path.parent.mkdir(parents=True, exist_ok=True)

    output = {
        "_defaults": _DEFAULTS,
        "variables": {},
    }
    for name, cfg in var_configs.items():
        # Only store non-default values to keep file concise
        overrides = {k: v for k, v in cfg.items() if _DEFAULTS.get(k) != v}
        if overrides:
            output["variables"][name] = overrides

    path.write_text(
        "# Auto-generated by pipeline. Edit and re-run to override.\n"
        + yaml.dump(output, default_flow_style=False, sort_keys=False),
        encoding="utf-8",
    )
    logger.info("Variable config saved to %s", path)


def _generate_llm_config(feature_names: list[str]) -> dict[str, dict]:
    """Call Claude Sonnet to generate per-variable binning config.

    [Assumption] LLM-assisted configuration is entirely our design.
    Uses Pydantic structured outputs for type-safe responses.
    """
    try:
        import anthropic
        from pydantic import BaseModel, Field
        from enum import Enum
        from typing import Literal
    except ImportError:
        logger.warning(
            "anthropic/pydantic not installed — falling back to defaults"
        )
        return {name: dict(_DEFAULTS) for name in feature_names}

    class MonotonicTrend(str, Enum):
        ascending = "ascending"
        descending = "descending"
        auto_asc_desc = "auto_asc_desc"
        peak = "peak"
        valley = "valley"

    class VariableBinningConfig(BaseModel):
        feature_name: str = Field(description="Exact DFS feature name")
        dtype: Literal["numerical", "categorical"] = Field(
            description="Data type"
        )
        monotonic_trend: MonotonicTrend = Field(
            description="Expected monotonic relationship with default risk"
        )
        max_n_bins: int = Field(ge=2, le=10, description="Maximum number of bins")
        rationale: str = Field(
            description="Brief credit risk rationale for the chosen monotonicity"
        )

    class VariableConfigResponse(BaseModel):
        variables: list[VariableBinningConfig]

    # Build prompt
    feature_list = "\n".join(f"- {name}" for name in feature_names[:100])
    prompt = (
        "You are a credit risk expert. For each DFS-generated feature below, "
        "infer the optimal binning parameters based on the feature name and "
        "its likely relationship with credit default risk.\n\n"
        f"Features:\n{feature_list}\n\n"
        "For each feature, provide: dtype (numerical/categorical), "
        "monotonic_trend (ascending if higher values = higher risk, "
        "descending if lower values = higher risk, auto_asc_desc if unsure), "
        "max_n_bins (2-10), and a brief rationale."
    )

    try:
        client = anthropic.Anthropic()
        response = client.messages.create(
            model=config.LLM_MODEL,
            max_tokens=4096,
            messages=[{"role": "user", "content": prompt}],
        )
        # Parse response — attempt structured extraction
        text = response.content[0].text
        parsed = VariableConfigResponse.model_validate_json(text)

        result: dict[str, dict] = {}
        for var in parsed.variables:
            result[var.feature_name] = {
                "dtype": var.dtype,
                "monotonic_trend": var.monotonic_trend.value,
                "max_n_bins": var.max_n_bins,
            }
        # Fill in any features not covered by LLM
        for name in feature_names:
            if name not in result:
                result[name] = dict(_DEFAULTS)

        logger.info("LLM config generated for %d features", len(result))
        return result

    except Exception as e:
        logger.warning("LLM config generation failed: %s — using defaults", e)
        return {name: dict(_DEFAULTS) for name in feature_names}
